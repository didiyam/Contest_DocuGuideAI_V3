"""
Windowsìš© ì•ˆì „ ì±—ë´‡ RAG (SQLite ì˜êµ¬ ì €ì¥)
SentenceTransformer ì œê±° ë²„ì „
â†’ OpenAI Embedding(text-embedding-3-small) ì‚¬ìš©
"""
 
import os
import uuid
import json
import pickle
import sqlite3
import numpy as np
from typing import List, Dict, Any
 
from openai import OpenAI
client = OpenAI()
 
# =====================================
# 0) ê²½ë¡œ ì„¤ì • ë° DB ì´ˆê¸°í™”
# =====================================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
STORAGE_DIR = os.path.join(BASE_DIR, "storage")
os.makedirs(STORAGE_DIR, exist_ok=True)
 
DB_PATH = os.path.join(STORAGE_DIR, "rag_db.sqlite")
 
 
def get_conn():
    """SQLite Connection"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn
 
 
def init_db():
    """embeddings í…Œì´ë¸” ìƒì„±"""
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS embeddings (
                id TEXT PRIMARY KEY,
                doc_id TEXT NOT NULL,
                type TEXT,
                page_num INTEGER,
                text TEXT,
                embedding BLOB NOT NULL,
                metadata TEXT
            )
            """
        )
        conn.commit()
 
 
# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì‹¤í–‰
init_db()
 
# =====================================
# 1) OpenAI Embedding í•¨ìˆ˜
# =====================================
def embed_text(text: str) -> np.ndarray:
    """
    OpenAI text-embedding-3-small ì‚¬ìš©
    SentenceTransformer ëŒ€ì²´
    """
    if not text:
        text = ""
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding, dtype=np.float32)
    except Exception as e:
        print("âŒ Embedding Error:", e)
        # fallback: zero vector
        return np.zeros(1536, dtype=np.float32)
 
 
# =====================================
# 2) dict â†’ ë¬¸ì¥ ë³€í™˜
# =====================================
def dict_to_sentence(item: dict) -> str:
    parts = []
    if item.get("who"):
        parts.append(f"ì£¼ì²´: {item['who']}")
    if item.get("action"):
        parts.append(f"í–‰ë™: {item['action']}")
    if item.get("when"):
        parts.append(f"ê¸°í•œ: {item['when']}")
    if item.get("how"):
        parts.append(f"ë°©ë²•: {item['how']}")
    if item.get("where"):
        parts.append(f"ì¥ì†Œ: {item['where']}")
    return " / ".join(parts)
 
 
# =====================================
# 3) (ë ˆê±°ì‹œ) ë©”ëª¨ë¦¬ vector DB â€” ìœ ì§€
# =====================================
vector_db: List[Dict[str, Any]] = []
 
 
def insert_actions(action_list):
    """í…ŒìŠ¤íŠ¸ìš© in-memory ì €ì¥ â€” RAGì™€ëŠ” ë¬´ê´€"""
    for item in action_list:
        sentence = dict_to_sentence(item)
        embedding_vec = embed_text(sentence).tolist()
 
        vector_db.append(
            {
                "id": str(uuid.uuid4()),
                "embedding": embedding_vec,
                "text": sentence,
                "metadata": item,
            }
        )
    print(f"[ë ˆê±°ì‹œ] ë©”ëª¨ë¦¬ DBì— {len(action_list)}ê°œ ì €ì¥ ì™„ë£Œ")
 
 
# =====================================
# 4) ë³¸ ê¸°ëŠ¥: doc_id ë‹¨ìœ„ë¡œ SQLiteì— ì €ì¥
# =====================================
def insert_info(doc_id: str, action_info: list, refined_txt: List[str]):
    """
    RAG ì €ì¥ í•¨ìˆ˜: í–‰ë™(action) + í˜ì´ì§€ í…ìŠ¤íŠ¸(refined_txt)
    """
    with get_conn() as conn:
        cur = conn.cursor()
        inserted = 0
 
        # 1) í–‰ë™(action_info)
        if action_info:
            for item in action_info:
                sentence = dict_to_sentence(item)
                if not sentence.strip():
                    continue
 
                embedding_vec = embed_text(sentence)
                emb_blob = pickle.dumps(embedding_vec)
 
                cur.execute(
                    """
                    INSERT INTO embeddings (id, doc_id, type, page_num, text, embedding, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        uuid.uuid4().hex,
                        doc_id,
                        "action",
                        None,
                        sentence,
                        emb_blob,
                        json.dumps(item, ensure_ascii=False),
                    ),
                )
                inserted += 1
 
        # 2) refined_txt (í˜ì´ì§€ í…ìŠ¤íŠ¸)
        if refined_txt:
            for idx, page_text in enumerate(refined_txt):
                clean_text = (page_text or "").strip()
                if not clean_text:
                    continue
 
                embedding_vec = embed_text(clean_text)
                emb_blob = pickle.dumps(embedding_vec)
 
                cur.execute(
                    """
                    INSERT INTO embeddings (id, doc_id, type, page_num, text, embedding, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        uuid.uuid4().hex,
                        doc_id,
                        "page",
                        idx + 1,
                        clean_text,
                        emb_blob,
                        json.dumps({"page": idx + 1}, ensure_ascii=False),
                    ),
                )
                inserted += 1
 
        conn.commit()
 
    print(f"ğŸ“Œ [SQLite] doc_id={doc_id} â†’ {inserted}ê°œ í•­ëª© ì €ì¥ ì™„ë£Œ")
 
 
# =====================================
# 5) (ë ˆê±°ì‹œ) in-memory ê²€ìƒ‰
# =====================================
def search_actions(query: str, top_k: int = 3):
    """í…ŒìŠ¤íŠ¸ìš© in-memory vector DB ê²€ìƒ‰"""
    if not vector_db:
        return []
 
    query_vec = embed_text(query)
 
    def cosine(a, b):
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))
 
    scores = [(cosine(query_vec, np.array(item["embedding"])), item) for item in vector_db]
    scores.sort(key=lambda x: x[0], reverse=True)
 
    return [item for score, item in scores[:top_k]]
 
 
# =====================================
# 6) SQLite ê¸°ë°˜ RAG ê²€ìƒ‰
# =====================================
def search_rag(doc_id: str, query: str, top_k: int = 5):
    """
    ë¬¸ì„œ ë‹¨ìœ„ RAG ê²€ìƒ‰
    action_info + refined_txt ì „ì²´ ê²€ìƒ‰
    """
    query_vec = embed_text(query)
 
    # DBì—ì„œ doc_id í•´ë‹¹í•˜ëŠ” embedding ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT id, doc_id, type, page_num, text, embedding, metadata
            FROM embeddings
            WHERE doc_id = ?
            """,
            (doc_id,),
        )
        rows = cur.fetchall()
 
    if not rows:
        print(f"âš ï¸ doc_id={doc_id} ë°ì´í„° ì—†ìŒ")
        return []
 
    def cosine(a, b):
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))
 
    scored_items = []
    for row in rows:
        emb_vec = pickle.loads(row["embedding"])  # np.array(float32)
        score = cosine(query_vec, emb_vec)
 
        item = {
            "id": row["id"],
            "doc_id": row["doc_id"],
            "type": row["type"],
            "page_num": row["page_num"],
            "text": row["text"],
            "embedding": emb_vec.tolist(),
            "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
            "score": score,
        }
        scored_items.append((score, item))
 
    scored_items.sort(key=lambda x: x[0], reverse=True)
 
    return [item for score, item in scored_items[:top_k]]
 
 
# =====================================
# 7) ê°„ë‹¨ ë‹µë³€ ìƒì„± ì˜ˆì‹œ
# =====================================
def generate_answer(user_query: str):
    """ë ˆê±°ì‹œ in-memory ê²€ìƒ‰ ì˜ˆì‹œ"""
    items = search_actions(user_query)
    if not items:
        return "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
 
    msg = "ê´€ë ¨ëœ ì•ˆë‚´ì…ë‹ˆë‹¤:\n"
    for it in items:
        msg += f"- {it['text']}\n"
    return msg
"""
RAG ê¸°ë°˜ ì•ˆì „ ì±—ë´‡ ì—”ì§„
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ ë²¡í„°DB ê²€ìƒ‰ ìˆ˜í–‰
- ê²€ìƒ‰ëœ ê·¼ê±° ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
- ë‹µë³€ ê¸°ë¡(state) ê´€ë¦¬ (ìµœëŒ€ 3ê°œ ìŠ¬ë¡¯)
- ì´ˆê³¼ ì‹œ ìë™ ìš”ì•½ ì €ì¥
"""
from src.chatbot.rag_builder import search_rag, embed_text # ì´ì§„ì•„ ì¶”ê°€ (ë²¡í„°DB ê²€ìƒ‰ ëª¨ë“ˆ)
from openai import OpenAI
from src.utils.config import load_api_keys


# "gpt-5.1-chat-latest" ì¨ì„œ ì œì¶œí•˜ê¸°
# -------------------------------
# call_llm_chat êµ¬í˜„ (ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡)
# -------------------------------
def call_llm_chat(prompt: str) -> str:
    """
    ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡ ì‘ë‹µ ìƒì„±ìš© LLM í˜¸ì¶œ í•¨ìˆ˜
    - JSON ì¶œë ¥ ê°•ì œ ì—†ìŒ
    - ìì—°ì–´ ë‹µë³€ + bullet ê°€ëŠ¥
    """
    API_KEY = load_api_keys()
    client = OpenAI(api_key=API_KEY)   

    resp = client.chat.completions.create(
        model="gpt-5.1-chat-latest" ,
        messages=[
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” ê³µê³µë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì±—ë´‡ì´ì•¼. "
                    "ì ˆëŒ€ JSONìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ê³ , ìì—°ì–´ ë¬¸ì¥ ë˜ëŠ” bullet í˜•ì‹ìœ¼ë¡œë§Œ ë§í•´."
                )
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        # temperature=0.2
    )

    return resp.choices[0].message.content.strip()
# ------------------------------
# 1) state êµ¬ì¡° ì •ì˜
# ------------------------------
state = {
    "history": [],       # [{"question": str, "answer": str}]
    "summary": "",        # ì˜¤ë˜ëœ ê¸°ë¡ ìš”ì•½ ì €ì¥
    "present_answer": ""    # ì§„ì•„ë‹˜ ìš”ì²­ì‚¬í•­(2025-11-27)
}


# ------------------------------
# 2) ì˜¤ë˜ëœ ê¸°ë¡ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
# ------------------------------
def summarize_history(history_list, existing_summary=""):
    """
    ì§ˆë¬¸Â·ë‹µë³€ ê¸°ë¡(history)ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    LLMì„ ì´ìš©í•´ ê¸°ì¡´ summary + ì‚­ì œë˜ëŠ” ê¸°ë¡ ìš”ì•½
    """

    text_to_summarize = existing_summary + "\n".join(
        [f"Q: {item['question']}\nA: {item['answer']}" for item in history_list]
    )

    prompt = f"""
    ë‹¤ìŒ ëŒ€í™”ê¸°ë¡ì„ ì§§ê³  í•µì‹¬ë§Œ ë‚¨ë„ë¡ ìš”ì•½í•˜ì„¸ìš”.
    ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì±—ë´‡ ë‹µë³€ì˜ íë¦„ì´ ìœ ì§€ë˜ë„ë¡ ì •ë¦¬í•´ì£¼ì„¸ìš”.

    ëŒ€í™” ê¸°ë¡:
    {text_to_summarize}
    """

    return call_llm_chat(prompt).strip()
# -----------------------------------------------------------------------------------
import re
import numpy as np

# ë¬¸ì¥ ë¶„ë¦¬
def split_sentences(text: str):
    sentences = re.split(r'(?<=[.!?â€¦\n])\s+', text)
    return [s.strip() for s in sentences if len(s.strip()) > 5]


# ì½”ì‚¬ì¸ ìœ ì‚¬ë„
def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


# ì§ˆë¬¸ vs ë¬¸ì„œ ë¬¸ì¥ ìœ ì‚¬ë„ ê¸°ë°˜ ìµœì  ë¬¸ì¥ ì°¾ê¸°
def find_best_sentence(query: str, chunk_text: str):
    sentences = split_sentences(chunk_text)
    if not sentences:
        return None

    q_vec = embed_text(query)  # ê¸°ì¡´ RAG embedding í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    best_sent = None
    best_score = -1

    for sent in sentences:
        s_vec = embed_text(sent)
        sim = cosine_sim(q_vec, s_vec)
        if sim > best_score:
            best_score = sim
            best_sent = sent

    return best_sent

# ------------------------------
# 3) RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± í•¨ìˆ˜ (ë¬¸ì¥ ê¸°ë°˜ ê·¼ê±°ì¶”ì¶œ ì™„ì „í†µí•© ë²„ì „)
# ------------------------------
def generate_response(doc_id: str, user_query: str) -> dict:
    """
    ë¬¸ì„œ ë‹¨ìœ„(doc_id) ê¸°ë°˜ RAG ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„± â†’ state ì—…ë°ì´íŠ¸
    """

    # 1) RAG ê²€ìƒ‰ ì‹¤í–‰
    retrieved_items = search_rag(doc_id=doc_id, query=user_query)

    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìœ¼ë©´
    if not retrieved_items:
        answer = "ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\në” ë§ì€ ì •ë³´ëŠ” ë¬¸ì„œ ì¶œì²˜ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."

        state["present_answer"] = answer
        state["history"].append({"question": user_query, "answer": answer})

        return {
            "answer": answer,
            "source": None,
            "state": state
        }

    # ğŸ”¥ 1-1) ì¶œì²˜ìš© í›„ë³´ = page í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§ (action ì œì™¸)
    page_items = [item for item in retrieved_items if item.get("type") == "page"]

    # ë§Œì•½ pageê°€ ê²€ìƒ‰ ì•ˆ ëë‹¤ë©´ fallback (í˜„ì‹¤ì ìœ¼ë¡œ ê±°ì˜ ì—†ìŒ)
    if not page_items:
        page_items = retrieved_items


    # 2) db_find: LLMì—ê²Œ ë³´ì—¬ì¤„ ì „ì²´ í…ìŠ¤íŠ¸ (ê·¸ëŒ€ë¡œ ìœ ì§€)
    db_find = "\n".join([
        f"- ({item.get('type','unknown')}) {item.get('text','')}"
        for item in retrieved_items
    ])

    # 3) ì´ì „ ëŒ€í™” history ë°˜ì˜
    history_text = "\n".join([
        f"Q: {h['question']}\nA: {h['answer']}"
        for h in state["history"]
    ])

    # 4) LLM Prompt êµ¬ì„±
    prompt = f"""
    ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€ë§Œ ì œê³µí•˜ëŠ” ì•ˆì „ ì±—ë´‡ì…ë‹ˆë‹¤.
    ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” íë¦„ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    ë¶€ë“œëŸ½ê³  ë‹¤ì •í•œ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”.

    [ë¬¸ì„œ ID]
    {doc_id}

    [ì´ì „ ëŒ€í™” ìš”ì•½]
    {state["summary"]}

    [ìµœê·¼ ëŒ€í™” ê¸°ë¡]
    {history_text}

    [ë¬¸ì„œì—ì„œ ì°¾ì€ ê·¼ê±° ë‚´ìš©]
    {db_find}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_query}

    ì‘ë‹µ ê·œì¹™:
    - ë¬¸ì„œì— ê¸°ë°˜í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ë§í•˜ì§€ ë§ ê²ƒ
    - í—ˆìœ„ ì •ë³´ ê¸ˆì§€
    - ì²«ë¬¸ì¥ê³¼ ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ì œì™¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë¬¸ì¥ë“¤ì€ bullet ë˜ëŠ” ë‹¨ê³„í˜•ìœ¼ë¡œë§Œ ë‹µë³€ ì‘ì„±
    - ì²«ë¬¸ì¥ì€ ì§ˆë¬¸ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì‹œì‘í•  ê²ƒ
    - ë§ˆì§€ë§‰ë¬¸ì¥ì€ "ë” ê¶ê¸ˆí•˜ì‹  ì‚¬í•­ì´ ìˆìœ¼ì‹ ê°€ìš”?"ë¡œ ëë‚¼ê²ƒ 
    - ê° í•­ëª©ì€ ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ êµ¬ë¶„í•  ê²ƒ
    - í•œ ì¤„ì— í•˜ë‚˜ì˜ bulletë§Œ í¬í•¨í•  ê²ƒ
    - ë¬¸ì¥ì€ ì ˆëŒ€ë¡œ ë¶™ì—¬ì“°ì§€ ë§ ê²ƒ
    """

    # 5) LLM í˜¸ì¶œ
    answer = call_llm_chat(prompt).strip()


    # =====================================================
    # 6) ğŸ”¥ ë¬¸ì„œ ì¶œì²˜ ë¬¸ì¥ ê¸°ë°˜ ì¶”ì¶œ (action ì œì™¸)
    # =====================================================
    best_sentences = []

    for item in page_items:     # ğŸ”¥ refined_txtì—ì„œë§Œ ë½‘ëŠ”ë‹¤
        page_text = item.get("text", "")
        best_sentence = find_best_sentence(user_query, page_text)

        if best_sentence:
            best_sentences.append(f"- {best_sentence}")

    # fallback: ê²€ìƒ‰ëœ page ì¤‘ ì²« ë²ˆì§¸
    if not best_sentences:
        fallback = page_items[0].get("text", "")
        best_sentences.append(f"- {fallback}")

    real_source = "\n".join(best_sentences)


    # =====================================================
    # 7) state ì—…ë°ì´íŠ¸
    # =====================================================
    state["present_answer"] = answer
    state["history"].append({"question": user_query, "answer": answer})

    # 8) state history í¬ê¸° ì œí•œ
    if len(state["history"]) > 3:
        old_data = state["history"][:-3]
        state["history"] = state["history"][-3:]
        state["summary"] = summarize_history(old_data, state["summary"])

    return {
        "answer": answer,
        "source": real_source,
        "state": state
    }
# # ------------------------------
# # 3) RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± í•¨ìˆ˜ (ì´ì§„ì•„ ìˆ˜ì • : doc_id ê¸°ë°˜)
# # ------------------------------
# def generate_response(doc_id: str, user_query: str) -> dict:
#     """
#     ë¬¸ì„œ ë‹¨ìœ„(doc_id) ê¸°ë°˜ RAG ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„± â†’ state ì—…ë°ì´íŠ¸
#     """

#     # 1) ë²¡í„° ê²€ìƒ‰ (doc_id ê¸°ì¤€ í•„í„° ì ìš©)
#     retrieved_items = search_rag(doc_id=doc_id, query=user_query)

#     # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ
#     if not retrieved_items:
#         answer = "ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\në” ë§ì€ ì •ë³´ëŠ” ë¬¸ì„œ ì¶œì²˜ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."

#         state["present_answer"] = answer
#         state["history"].append({"question": user_query, "answer": answer})
#         return {
#             "answer": answer,
#             "source": None,
#             "state": state
#         }

#     # 2) db_find: ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ì¡°í•© (LLMìš©, ê·¸ëŒ€ë¡œ ìœ ì§€)
#     db_find = "\n".join([
#         f"- ({item.get('type','unknown')}) {item.get('text','')}"
#         for item in retrieved_items
#     ])


#     # 3) ê¸°ì¡´ history ë°˜ì˜
#     history_text = "\n".join([
#         f"Q: {h['question']}\nA: {h['answer']}"
#         for h in state["history"]
#     ])

#     # 4) í”„ë¡¬í”„íŠ¸ ìƒì„±
#     prompt = f"""
#     ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€ë§Œ ì œê³µí•˜ëŠ” ì•ˆì „ ì±—ë´‡ì…ë‹ˆë‹¤.
#     ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” íë¦„ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
#     ë¶€ë“œëŸ½ê³  ë‹¤ì •í•œ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”.

#     [ë¬¸ì„œ ID]
#     {doc_id}

#     [ì´ì „ ëŒ€í™” ìš”ì•½]
#     {state["summary"]}

#     [ìµœê·¼ ëŒ€í™” ê¸°ë¡]
#     {history_text}

#     [ë¬¸ì„œì—ì„œ ì°¾ì€ ê·¼ê±° ë‚´ìš©]
#     {db_find}

#     [ì‚¬ìš©ì ì§ˆë¬¸]
#     {user_query}

#     ì‘ë‹µ ê·œì¹™:
#     - ë¬¸ì„œì— ê¸°ë°˜í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ë§í•˜ì§€ ë§ ê²ƒ
#     - í—ˆìœ„ ì •ë³´ ê¸ˆì§€
#     - ì²«ë¬¸ì¥ê³¼ ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ì œì™¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë¬¸ì¥ë“¤ì€ bullet ë˜ëŠ” ë‹¨ê³„í˜•ìœ¼ë¡œë§Œ ë‹µë³€ ì‘ì„±
#     - ì²«ë¬¸ì¥ì€ ì§ˆë¬¸ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì‹œì‘í•  ê²ƒ
#     - ë§ˆì§€ë§‰ë¬¸ì¥ì€ "ë” ê¶ê¸ˆí•˜ì‹  ì‚¬í•­ì´ ìˆìœ¼ì‹ ê°€ìš”?"ë¡œ ëë‚¼ê²ƒ 
#     - ê° í•­ëª©ì€ ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ êµ¬ë¶„í•  ê²ƒ
#     - í•œ ì¤„ì— í•˜ë‚˜ì˜ bulletë§Œ í¬í•¨í•  ê²ƒ
#     - ë¬¸ì¥ì€ ì ˆëŒ€ë¡œ ë¶™ì—¬ì“°ì§€ ë§ ê²ƒ
#     - ì•„ë˜ ì˜ˆì‹œ í˜•ì‹ì„ ë°˜ë“œì‹œ ë”°ë¥¼ ê²ƒ:

#     ì˜ˆì‹œ í˜•ì‹:
#     ì²« ë²ˆì§¸ í•­ëª©
#     - ë‘ ë²ˆì§¸ í•­ëª©
#     - ì„¸ ë²ˆì§¸ í•­ëª©
#     ë” ê¶ê¸ˆí•˜ì‹  ì‚¬í•­ì´ ìˆìœ¼ì‹ ê°€ìš”?
#     """

#     # 5) LLM í˜¸ì¶œ
#     answer = call_llm_chat(prompt).strip()
    
#     # ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì¥ë§Œ ê·¼ê±°ë¡œ ì¶”ì¶œ
#     # --------------------------
#     used_sources = []

#     for item in retrieved_items:
#         text = item.get("text", "")
#         # ë¬¸ì¥ì„ 40~80 ê¸€ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ë§¤ì¹­ ì •í™•ë„ UP
#         key = text[:60]

#         if key in answer:   # ë‹µë³€ ë³¸ë¬¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°ë§Œ ê·¼ê±°ë¡œ ì¸ì •
#             used_sources.append(
#                 f"- ({item.get('type','unknown')}) {text}"
#             )

#     # ì•„ë¬´ ë§¤ì¹­ ì•ˆë˜ë©´ ê°€ì¥ ìƒìœ„ 1ê°œë§Œ fallback
#     if not used_sources:
#         top = retrieved_items[0]
#         used_sources.append(
#             f"- ({top.get('type','unknown')}) {top.get('text','')}"
#         )

#     real_source = "\n".join(used_sources)

#     # 6) state ì—…ë°ì´íŠ¸
#     state["present_answer"] = answer
#     state["history"].append({"question": user_query, "answer": answer})

#     # 7) state í¬ê¸° ê´€ë¦¬
#     if len(state["history"]) > 3:
#         old_data = state["history"][:-3]
#         state["history"] = state["history"][-3:]
#         state["summary"] = summarize_history(old_data, state["summary"])

#     return {
#         "answer": answer,
#         "source":real_source,
#         "state": state
#     }



# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# if __name__ == "__main__":
#     print("RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")

#     # í…ŒìŠ¤íŠ¸ìš© doc_id (ì›í•˜ëŠ” ê±¸ë¡œ ë°”ê¿”ë„ ë¨)
#     TEST_DOC_ID = "test-doc-1234"

#     while True:
#         user_q = input("\nì‚¬ìš©ì ì§ˆë¬¸: ")

#         if user_q.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
#             print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
#             break

#         # generate_response(doc_id, user_query)
#         result = generate_response(TEST_DOC_ID, user_q)

#         print("\nì±—ë´‡ ë‹µë³€:")
#         print(result["answer"])

#         print("\nê·¼ê±°(REAL SOURCE):")
#         print(result["source"])   # real_source ë“¤ì–´ìˆìŒ

#         print("\n-------------------------------------")



